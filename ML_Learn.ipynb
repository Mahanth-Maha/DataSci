{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "1. Supervised Learning\n",
    "   * Uni-Variate Linear Regression\n",
    "   * Multi-Variate Linear Regression\n",
    "   * Classification\n",
    "   \n",
    "2. Advanced Learning Algorithms\n",
    "\n",
    "3. Unsupervised,Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni-Variate Linear Regression\n",
    "* fitting line y = wx + b\n",
    "* model => \n",
    "  \n",
    "  ` y_pred[i] = f w,b( x[i] ) = w*x[i] + b `\n",
    "  \n",
    "* Cost Funtion \n",
    "\n",
    "  ` J(w,b) = 1/2m * SUMMATION( y_pred - y_train(i) )^2 `\n",
    "\n",
    "* update weights as\n",
    "\n",
    "``` \n",
    "  w = w - alpha * J(w,b)'w.r.t. w  \n",
    "    = w - (alp /m) * SUMMATION( w*x[i] + b  - y_train) * x[i] \n",
    "```\n",
    "\n",
    "``` \n",
    "  b = b - alpha * J(w,b)'w.r.t. b \n",
    "    = b - (alp /m) * SUMMATION( w*x[i] + b  - y_train) \n",
    "```\n",
    "\n",
    "* Cost minimization by BATCH GRADIENT DESCENT algorithm with fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(w,b) found by gradient descent: (199.9928,100.0116)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300.004429077974"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "import copy\n",
    "class Uni_Var_Linear_Regression:\n",
    "    def __init__(self,x_train = np.array([1.0, 2.0]) ,y_train = np.array([300.0, 500.0]), b_init = 0,w_init = 0,iterations = 100000,alpha = 0.001) -> None:\n",
    "        self.x_train = x_train  #features\n",
    "        self.y_train = y_train   #target value\n",
    "        self.w = w_init\n",
    "        self.b = b_init\n",
    "        self.iterations = iterations\n",
    "        self.alpha = alpha\n",
    "        self.w_final, self.b_final = [0,0]\n",
    "\n",
    "    #Function to calculate the cost\n",
    "    def compute_cost(self):\n",
    "        m = self.x_train.shape[0] \n",
    "        cost = 0\n",
    "        for i in range(m):\n",
    "            f_wb = self.w * self.x_train[i] + self.b\n",
    "            cost = cost + (f_wb - self.y_train[i])**2\n",
    "        total_cost = 1 / (2 * m) * cost\n",
    "        return total_cost\n",
    "\n",
    "    #Function to calculate the gradient\n",
    "    def compute_gradient(self): \n",
    "        m = self.x_train.shape[0]    \n",
    "        dj_dw = 0.0\n",
    "        dj_db = 0.0\n",
    "        for i in range(m):  \n",
    "            f_wb = self.w * self.x_train[i] + self.b \n",
    "            dj_dw_i = (f_wb - self.y_train[i]) * self.x_train[i] \n",
    "            dj_db_i = f_wb - self.y_train[i] \n",
    "            dj_db += dj_db_i\n",
    "            dj_dw += dj_dw_i \n",
    "        dj_dw = dj_dw / m \n",
    "        dj_db = dj_db / m         \n",
    "        return dj_dw, dj_db\n",
    "\n",
    "    #gradient descent algorithm\n",
    "    def gradient_descent(self): \n",
    "        w = copy.deepcopy(self.w) # avoid modifying global w_in\n",
    "        for i in range(self.iterations):\n",
    "            # Calculate the gradient and update the parameters using gradient_function\n",
    "            dj_dw, dj_db = self.compute_gradient()\n",
    "            # Update Parameters using equation (3) above\n",
    "            self.b = self.b - self.alpha * dj_db\n",
    "            self.w = self.w - self.alpha * dj_dw\n",
    "            # Save cost J at each iteration\n",
    "            if i < 100000:      # prevent resource exhaustion \n",
    "                self.compute_cost()\n",
    "        # return w, b\n",
    "        self.w_final = self.w\n",
    "        self.b_final = self.b\n",
    "        print(f\"(w,b) found by gradient descent: ({self.w_final:8.4f},{self.b_final:8.4f})\")\n",
    "\n",
    "    def predict (self,xi):\n",
    "        return self.w_final*(xi/1000) +self.b_final\n",
    "\n",
    "LinReg_default = Uni_Var_Linear_Regression()\n",
    "LinReg_default.gradient_descent()\n",
    "LinReg_default.predict(1000) #  predict for 1000 ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Variate Linear Regression\n",
    "\n",
    "---------WRONG----------\n",
    "* fitting line y = wx + b\n",
    "* model => \n",
    "  \n",
    "  ` y_pred[i] = f w,b( x[i] ) = w*x[i] + b `\n",
    "  \n",
    "* Cost Funtion \n",
    "\n",
    "  ` J(w,b) = 1/2m * SUMMATION( y_pred - y_train(i) )^2 `\n",
    "\n",
    "* update weights as\n",
    "\n",
    "``` \n",
    "  w = w - alpha * J(w,b)'w.r.t. w  \n",
    "    = w - (alp /m) * SUMMATION( w*x[i] + b  - y_train) * x[i] \n",
    "```\n",
    "\n",
    "``` \n",
    "  b = b - alpha * J(w,b)'w.r.t. b \n",
    "    = b - (alp /m) * SUMMATION( w*x[i] + b  - y_train) \n",
    "```\n",
    "\n",
    "* Cost minimization by BATCH GRADIENT DESCENT algorithm with fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Learn",
   "language": "python",
   "name": "ml-learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a075be670f002415fc65291a584e903cef0587159d6bdf77e827b87cdcbb0bd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
